# rag_kitcore

A modular, extensible Retrievalâ€‘Augmented Generation (RAG) toolkit designed for clarity, maintainability, and productionâ€‘grade workflows.  
The project follows a clean architecture with separate subsystems for:

- **Indexing** (document ingestion, conversion, cleaning, chunking, embedding, vector store population)
- **Retrieval** (hybrid search, reranking, query pipelines)
- **LLM orchestration** (prompting, backends, inference)
- **Configuration** (typed settings, YAML + .env support)

This repository uses a `src/` layout and Poetry for dependency management.

---

## ðŸ“¦ Installation

```bash
poetry install
poetry shell
```

---
## ðŸ“š Subsystem Documentation
### Indexing Pipeline
Documentation for the indexing subsystem is available at:

```Code
src/rag_kitcore/rag/indexing/README.md
```
This includes:

* architecture overview
* module responsibilities
* configuration structure
* how to run the indexing pipeline
* how to extend converters, chunkers, embedders, and vector stores

---

## ðŸš€ Running the Indexing Pipeline
```bash
python -m rag_kitcore.rag.indexing
```
or via the CLI:

```bash
rag-index
```
---

## ðŸ—‚ Project Structure
```Code
rag_kitcore/
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ prompts.yaml
â””â”€â”€ src/
    â””â”€â”€ rag_kitcore/
        â”œâ”€â”€ logsys/
        â”œâ”€â”€ rag/
        â”‚   â””â”€â”€ conversion/
        â”‚   â””â”€â”€ embedding/
        â”‚   â””â”€â”€ indexing/
        â”‚   â””â”€â”€ pipelines/
        â”‚   â””â”€â”€ prompts/
        â”‚   â””â”€â”€ retrievers/
        â””â”€â”€ core/
            â””â”€â”€ bm_25/
            â””â”€â”€ device/
            â””â”€â”€ embeddings/
            â””â”€â”€ llm/
            â””â”€â”€ rerankers/
            â””â”€â”€ vectorstore/
            â””â”€â”€ exceptions.py
            â””â”€â”€ settings.py
```

---

## ðŸ§­ Status
Indexing subsystem: **complete and stable**
Retrieval subsystem: **in progress**  
LLM orchestration: **in progress**

---

## ðŸ”¢ ML Stack Versions (Pinned)

The backend uses a fully pinned ML stack to ensure reproducible builds and stable behavior across CPU and GPU environments.

| Component               | Version     | Notes |
|-------------------------|-------------|-------|
| PyTorch (CPU)           | 2.2.0       | Official CPU wheels |
| Transformers            | 4.37.2      | Compatible with ST 2.2.2 |
| Tokenizers              | 0.15.1      | Matches Transformers 4.37 |
| SentenceTransformers    | 2.2.2       | Stable, CrossEncoder supported |
| HuggingFace Hub         | 0.19.4      | Required for ST 2.2.2 (`cached_download`) |
| SentencePiece           | 0.1.99      | Required for HF models |
| scikit-learn            | 1.3.2       | Required for ST |
| nltk                    | 3.8.1       | Required for ST |
| Qdrant Client           | 1.7.3       | Matches Qdrant 1.7.x |
| FastAPI                 | 0.109.0     | Backend API |
| Uvicorn                 | 0.27.0      | ASGI server |

This stack is intentionally conservative to ensure stability during development.  
A future upgrade to SentenceTransformers 3.x will be handled separately.
