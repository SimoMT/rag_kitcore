services:
  webapp:
    build: .
    ports:
      - "8000:8000"
    environment:
      VECTOR_DB: "qdrant"
      LLM_PROVIDER: "ollama"
      OLLAMA_HOST: "host.docker.internal"
      OLLAMA_PORT: "11434"
    volumes:
      - ./data:/app/data
    depends_on:
      - qdrant
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:v1.16
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  # vllm:
  #   image: rocm/vllm-openai:latest
  #   environment:
  #     - VLLM_CPU_ONLY=1
  #   command: >
  #     --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
  #     --dtype float32
  #     --port 8001
  #   ports:
  #     - "8001:8001"
  #   restart: unless-stopped

volumes:
  qdrant_data: